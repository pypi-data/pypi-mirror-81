Metadata-Version: 2.1
Name: adapter-transformers
Version: 1.0.1
Summary: A friendly fork of Huggingface's Transformers, adding Adapters to PyTorch language models
Home-page: https://github.com/adapter-hub/adapter-transformers
Author: Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, based on work by Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors
Author-email: pfeiffer@ukp.tu-darmstadt.de
License: Apache
Keywords: NLP deep learning transformer pytorch BERT adapters
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.6.0
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: tokenizers (==0.7.0)
Requires-Dist: packaging
Requires-Dist: filelock
Requires-Dist: requests
Requires-Dist: tqdm (>=4.27)
Requires-Dist: regex (!=2019.12.17)
Requires-Dist: sentencepiece
Requires-Dist: sacremoses
Requires-Dist: dataclasses ; python_version < "3.7"
Provides-Extra: all
Requires-Dist: pydantic ; extra == 'all'
Requires-Dist: uvicorn ; extra == 'all'
Requires-Dist: fastapi ; extra == 'all'
Requires-Dist: starlette ; extra == 'all'
Requires-Dist: tensorflow ; extra == 'all'
Requires-Dist: torch ; extra == 'all'
Provides-Extra: dev
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: pytest-xdist ; extra == 'dev'
Requires-Dist: timeout-decorator ; extra == 'dev'
Requires-Dist: pytest-subtests ; extra == 'dev'
Requires-Dist: black (==19.10b0) ; extra == 'dev'
Requires-Dist: isort ; extra == 'dev'
Requires-Dist: flake8 ; extra == 'dev'
Requires-Dist: mecab-python3 ; extra == 'dev'
Requires-Dist: scikit-learn ; extra == 'dev'
Requires-Dist: tensorflow ; extra == 'dev'
Requires-Dist: torch ; extra == 'dev'
Provides-Extra: docs
Requires-Dist: recommonmark ; extra == 'docs'
Requires-Dist: sphinx ; extra == 'docs'
Requires-Dist: sphinx-markdown-tables ; extra == 'docs'
Requires-Dist: sphinx-rtd-theme ; extra == 'docs'
Provides-Extra: mecab
Requires-Dist: mecab-python3 ; extra == 'mecab'
Provides-Extra: quality
Requires-Dist: black (==19.10b0) ; extra == 'quality'
Requires-Dist: isort ; extra == 'quality'
Requires-Dist: flake8 ; extra == 'quality'
Provides-Extra: serving
Requires-Dist: pydantic ; extra == 'serving'
Requires-Dist: uvicorn ; extra == 'serving'
Requires-Dist: fastapi ; extra == 'serving'
Requires-Dist: starlette ; extra == 'serving'
Provides-Extra: sklearn
Requires-Dist: scikit-learn ; extra == 'sklearn'
Provides-Extra: testing
Requires-Dist: pytest ; extra == 'testing'
Requires-Dist: pytest-xdist ; extra == 'testing'
Requires-Dist: timeout-decorator ; extra == 'testing'
Requires-Dist: pytest-subtests ; extra == 'testing'
Provides-Extra: tf
Requires-Dist: tensorflow ; extra == 'tf'
Requires-Dist: onnxconverter-common ; extra == 'tf'
Requires-Dist: keras2onnx ; extra == 'tf'
Provides-Extra: tf-cpu
Requires-Dist: tensorflow-cpu ; extra == 'tf-cpu'
Requires-Dist: onnxconverter-common ; extra == 'tf-cpu'
Requires-Dist: keras2onnx ; extra == 'tf-cpu'
Provides-Extra: torch
Requires-Dist: torch ; extra == 'torch'

<p align="center">
<img style="vertical-align:middle" src="https://raw.githubusercontent.com/Adapter-Hub/adapter-transformers/master/adapter_docs/logo.png" />
</p>
<h1 align="center">
<span>adapter-transformers</span>
</h1>

<h3 align="center">
A friendly fork of HuggingFace's <i>Transformers</i>, adding Adapters to PyTorch language models
</h3>

![Tests](https://github.com/Adapter-Hub/adapter-transformers/workflows/Tests/badge.svg)
[![GitHub](https://img.shields.io/github/license/adapter-hub/adapter-transformers.svg?color=blue)](https://github.com/adapter-hub/adapter-transformers/blob/master/LICENSE)
![PyPI](https://img.shields.io/pypi/v/adapter-transformers)

`adapter-transformers` is an extension of [HuggingFace's Transformers](https://github.com/huggingface/transformers) library, integrating adapters into state-of-the-art language models by incorporating **[AdapterHub](https://adapterhub.ml)**, a central repository for pre-trained adapter modules.

This library can be used as a drop-in replacement for HuggingFace Transformers and regularly synchronizes new upstream changes.

## Installation

_adapter-transformers_ currently supports **Python 3.6+** and **PyTorch 1.1.0+**.
After [installing PyTorch](https://pytorch.org/get-started/locally/), you can install _adapter-transformers_ from PyPI ...

```
pip install -U adapter-transformers
```

... or from source by cloning the repository:

```
git clone https://github.com/adapter-hub/adapter-transformers.git
cd adapter-transformers
pip install .
```

## Getting Started

HuggingFace's great documentation on getting started with _Transformers_ can be found [here](https://huggingface.co/transformers/index.html). _adapter-transformers_ is fully compatible with _Transformers_.

To get started with adapters, refer to these locations:

- **https://docs.adapterhub.ml**, our documentation on training and using adapters with _adapter-transformers_
- **https://adapterhub.ml** to explore available pre-trained adapter modules and share your own adapters
- **[Examples folder](https://github.com/Adapter-Hub/adapter-transformers/tree/master/examples)** of this repository containing HuggingFace's example training scripts, many adapted for training adapters


## Citation

If you find this library useful, please cite our paper [AdapterHub: A Framework for Adapting Transformers](https://arxiv.org/abs/2007.07779):

```
@article{pfeiffer2020AdapterHub,
    title={AdapterHub: A Framework for Adapting Transformers},
    author={Jonas Pfeiffer and
            Andreas R\"uckl\'{e} and
            Clifton Poth and
            Aishwarya Kamath and
            Ivan Vuli\'{c} and
            Sebastian Ruder and
            Kyunghyun Cho and
            Iryna Gurevych},
    journal={arXiv preprint},
    year={2020},
    url={https://arxiv.org/abs/2007.07779}
}
```


