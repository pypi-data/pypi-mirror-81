[databand]

[config]
validate_no_extra_params = warn

[core]
environments = ['local']
sql_alchemy_conn = sqlite:///${DBND_SYSTEM}/dbnd.db

tracker = ['console', 'file', 'api']

tracker_api = web
tracker_version = 2

databand_url =

# standalone config
fix_env_on_osx = True

# engines configuration
[local_machine_engine]
_type = dbnd._core.settings.engine.LocalMachineEngineConfig

[docker]
_type = dbnd_docker.docker.docker_engine_config.DockerEngineConfig
network =
sql_alchemy_conn =


[kubernetes]
_type = dbnd_docker.kubernetes.kubernetes_engine_config.KubernetesEngineConfig

# assumes a Kubernetes deployement using our deploy script with an in cluster Postgres and a secret with these names
system_secrets = [{ "type":"env", "target": "AIRFLOW__CORE__SQL_ALCHEMY_CONN", "secret" : "databand-secrets" , "key" :"sql_alchemy_conn"},
                 { "type":"env", "target": "DBND__CORE__SQL_ALCHEMY_CONN", "secret" : "databand-secrets" , "key" :"sql_alchemy_conn"},
                 { "type":"env", "target": "DBND__CORE__DATABAND_URL", "secret" : "databand-secrets" , "key" :"databand_url"},
                 { "type":"env", "target": "AIRFLOW__CORE__FERNET_KEY", "secret" : "databand-secrets" , "key" :"fernet_key"}]

submit_termination_grace_period = 30s

# environment configurations
[local]
root = ${DBND_HOME}/data
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local

[gcp]
_type = dbnd_gcp.env.GcpEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd

conn_id = google_cloud_default

spark_engine = dataproc

[aws]
_type = dbnd_aws.env.AwsEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local
; spark_engine = emr
docker_engine = aws_batch

[azure]
_type = dbnd_azure.env.AzureCloudConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd


# spark configurations
[spark]
_type = dbnd_spark.spark_config.SparkConfig

[livy]
_type = livy

[spark_local]
_type = dbnd_spark.local.local_spark_config.SparkLocalEngineConfig
conn_id = spark_default
enable_spark_context_inplace = True


[dataproc]
_type = dbnd_gcp.dataproc.dataproc_config.DataprocConfig

[databricks]
_type = dbnd_databricks.databricks_config.DatabricksConfig
conn_id = databricks_default


[qubole]
_type = dbnd_qubole.qubole_config.QuboleConfig

[databricks_azure]
local_dbfs_mount = /mnt/dbnd/

[emr]
_type = emr


[output]
path_task = {root}{sep}{env_label}{sep}{task_target_date}{sep}{task_name}{sep}{task_name}{task_class_version}_{task_signature}{sep}{output_name}{output_ext}
path_prod_immutable_task = {root}{sep}production{sep}{task_name}{task_class_version}{sep}{output_name}{output_ext}{sep}date={task_target_date}

target = csv
str = txt
object = pickle
List[object] = pickle
List[str] = csv
Dict[Any,DataFrame] = pickle
pandas_dataframe = csv
tensorflow_model = tfmodel
tensorflow_history = tfhistory

pandas_df_dict = hdf5
numpy_ndarray = numpy
matplotlib_figure = png
spark_dataframe = csv

hdf_format = fixed

validate_no_extra_params = disabled

[task]
task_env = local
task_target_date = today
task_version = 1
task_enabled = True
task_enabled_in_prod = True

task_in_memory_outputs = False
task_is_dynamic = False

task_supports_dynamic_tasks = True

task_retries = 0
task_retry_delay = 1s

validate_no_extra_params = error

[run]
heartbeat_timeout_s = 900
heartbeat_interval_s = 5
heartbeat_sender_log_to_file = True

[log]
# Logging level
level = INFO

# Logging format
formatter = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
formatter_simple = %%(asctime)s %%(levelname)s - %%(message)s
formatter_colorlog = [%%(asctime)s] %%(log_color)s%%(levelname)s %%(reset)s %%(task)-15s - %%(message)s

console_formatter_name = formatter_colorlog
file_formatter_name = formatter

sentry_url =

at_warn = azure.storage,flask_appbuilder

[airflow]
sql_alchemy_conn = dbnd
fernet_key = dbnd

auto_add_versioned_dags = True
auto_add_scheduled_dags = True
auto_disable_scheduled_dags_load = True

optimize_airflow_db_access = True
disable_db_ping_on_connect = True
disable_dag_concurrency_rules = True

dbnd_dag_concurrency = 100000

webserver_url = http://localhost:8082

use_connections = True

[scheduler]
config_file = ${DBND_SYSTEM}/scheduler.yml
default_retries = 3
refresh_interval = 10
active_by_default = True
shell_cmd = True

# credentials to connect to the databand webserver
dbnd_user = databand
dbnd_password = databand

[webserver]
web_server_host = 0.0.0.0
web_server_port = 8081
web_server_master_timeout = 120
web_server_worker_timeout = 120
workers = 4
worker_class = sync
worker_refresh_batch_size = 0
worker_refresh_interval = 30
access_logfile = -
error_logfile = -
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s "%%(f)s"

# Secret key used to run your flask app
secret_key = temporary_key

# TODO: fill in the config object with all the stuff we have here
validate_no_extra_params = disabled

[airflow_monitor]
interval = 10
fetcher = web
;fetcher = db
include_logs = False
tasks_per_fetch = 100

# DAGs to monitor, if not defined - monitors all DAGs
;dag_ids = ['ingest_data_dag', 'simple_dag']

airflow_url = http://localhost:8080

# For 'fetcher = composer' mode
;airflow_url = https://composer-airflow-tp.appspot.com
composer_client_id = client-id.apps.googleusercontent.com

# For 'fetcher = db' mode
local_dag_folder =  /usr/local/airflow/dags
sql_alchemy_conn = sqlite:////usr/local/airflow/airflow.db
rbac_enabled = False
; sql_alchemy_conn = postgresql+psycopg2://postgres:airflow@localhost:5432/airflow
